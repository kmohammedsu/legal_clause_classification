{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 â€“ Data Preprocessing and Preparation\n",
    "\n",
    "**Run order:** Third (after 00_download_cuad and 02_eda). Requires CUAD data.\n",
    "\n",
    "This notebook cleans the CUAD clause data, encodes labels, splits train/test, and prepares TF-IDF and sequence features.\n",
    "**Output:** Clean dataset and model-ready arrays under `data/processed/`. Next: run **04_legal_clause_classification.ipynb** to train models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and load clause data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUAD dataset directory already exists.\n",
      "\n",
      "Dataset found at: /Users/khajamoinuddinmohammed/Documents/MSDS/FALL 2025/BUAN 5312 ADVANCED ML/final project/cuad/data\n",
      "\n",
      "Contents of data directory:\n",
      "   CUADv1.json\n",
      "   test.json\n",
      "   train_separate_questions.json\n",
      "EXTRACTING CLAUSES FROM CUADv1.json\n",
      "Processing 510 contracts...\n",
      "\n",
      "[OK] Extracted 13823 clauses from 510 contracts\n",
      "   Unique categories: 41\n",
      "   Unique contracts: 510\n",
      "Loaded 13823 clauses. Using data at: /Users/khajamoinuddinmohammed/Documents/MSDS/FALL 2025/BUAN 5312 ADVANCED ML/final project/cuad/data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths: run from project root or Notebooks/\n",
    "project_root = Path.cwd() if (Path.cwd() / \"scripts\").exists() else Path.cwd().parent\n",
    "import sys\n",
    "sys.path.insert(0, str(project_root))\n",
    "from scripts.download_cuad import ensure_cuad_data\n",
    "from scripts.load_cuad_clauses import get_clauses_df, extract_clauses_from_cuadv1\n",
    "\n",
    "cuad_path, data_path = ensure_cuad_data(project_root)\n",
    "clauses_df = get_clauses_df(data_path)\n",
    "print(f\"Loaded {len(clauses_df)} clauses. Using data at: {data_path.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Preparation\n",
    "\n",
    "Prepare data for **multi-class classification** (softmax output) for both models:\n",
    "\n",
    "**Model 1: Feedforward Neural Network (MLP) with TF-IDF**\n",
    "- Convert clauses to TF-IDF vectors\n",
    "- 1-2 hidden layers with ReLU\n",
    "- Softmax output for multi-class classification\n",
    "- Sparse Categorical Crossentropy loss, Adam optimizer\n",
    "\n",
    "**Model 2: LSTM-Based Text Classifier**\n",
    "- Tokenize into sequences of word IDs\n",
    "- Random embeddings (can use pretrained later)\n",
    "- LSTM/BiLSTM to capture word order\n",
    "- Softmax output for multi-class classification\n",
    "- Sparse Categorical Crossentropy loss, Adam optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initial Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CLEANING AND PREPARATION\n",
      "\n",
      "Initial dataset size: 13823 clauses\n",
      "After removing empty/very short clauses: 12883 clauses (removed 940)\n",
      "After removing duplicate clause texts: 11047 clauses\n",
      "\n",
      "[OK] All categories have >= 5 samples\n",
      "\n",
      "Final cleaned dataset size: 11047 clauses\n",
      "Unique categories: 41\n",
      "\n",
      "CATEGORY DISTRIBUTION (After Cleaning):\n",
      "category\n",
      "Parties                      1577\n",
      "License Grant                 658\n",
      "Audit Rights                  633\n",
      "Anti-Assignment               614\n",
      "Insurance                     550\n",
      "Cap On Liability              545\n",
      "Governing Law                 453\n",
      "Agreement Date                434\n",
      "Expiration Date               415\n",
      "Revenue/Profit Sharing        414\n",
      "Post-Termination Services     410\n",
      "Exclusivity                   402\n",
      "Minimum Commitment            398\n",
      "Rofr/Rofo/Rofn                353\n",
      "Ip Ownership Assignment       310\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA CLEANING AND PREPARATION\")\n",
    "\n",
    "df_clean = clauses_df.copy()\n",
    "\n",
    "print(f\"\\nInitial dataset size: {len(df_clean)} clauses\")\n",
    "\n",
    "initial_size = len(df_clean)\n",
    "df_clean = df_clean[df_clean['clause_text'].str.strip() != '']\n",
    "df_clean = df_clean[df_clean['word_count'] >= 2]\n",
    "print(f\"After removing empty/very short clauses: {len(df_clean)} clauses (removed {initial_size - len(df_clean)})\")\n",
    "\n",
    "df_clean = df_clean.drop_duplicates(subset=['clause_text'], keep='first')\n",
    "print(f\"After removing duplicate clause texts: {len(df_clean)} clauses\")\n",
    "\n",
    "min_samples_per_category = 5\n",
    "category_counts_clean = df_clean['category'].value_counts()\n",
    "rare_categories = category_counts_clean[category_counts_clean < min_samples_per_category].index\n",
    "\n",
    "if len(rare_categories) > 0:\n",
    "    print(f\"\\n[WARNING]  Found {len(rare_categories)} categories with < {min_samples_per_category} samples:\")\n",
    "    for cat in rare_categories:\n",
    "        print(f\"   - {cat}: {category_counts_clean[cat]} samples\")\n",
    "    print(f\"   -> Keeping rare categories (will use class weights in models)\")\n",
    "else:\n",
    "    print(f\"\\n[OK] All categories have >= {min_samples_per_category} samples\")\n",
    "\n",
    "print(f\"\\nFinal cleaned dataset size: {len(df_clean)} clauses\")\n",
    "print(f\"Unique categories: {df_clean['category'].nunique()}\")\n",
    "\n",
    "\n",
    "print(\"\\nCATEGORY DISTRIBUTION (After Cleaning):\")\n",
    "print(df_clean['category'].value_counts().head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text Preprocessing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLYING TEXT PREPROCESSING\n",
      "\n",
      "[OK] Preprocessing applied to 11047 clauses\n",
      "\n",
      "Sample original vs processed:\n",
      "\n",
      "  Original:  DISTRIBUTOR AGREEMENT...\n",
      "  Processed: distributor agreement...\n",
      "\n",
      "  Original:  Electric City Corp....\n",
      "  Processed: electric city corp....\n",
      "\n",
      "  Original:  Electric City of Illinois L.L.C....\n",
      "  Processed: electric city of illinois l.l.c....\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text, remove_numbers=False, remove_punctuation=False):\n",
    "    \"\"\"Clean text for preprocessing.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "\n",
    "    text = str(text).strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text_for_tfidf(text):\n",
    "    \"\"\"Preprocess text for TF-IDF vectorization (Feedforward model).\"\"\"\n",
    "    return clean_text(text, remove_numbers=False, remove_punctuation=False)\n",
    "\n",
    "def preprocess_text_for_lstm(text):\n",
    "    \"\"\"Preprocess text for LSTM model.\"\"\"\n",
    "    return clean_text(text, remove_numbers=False, remove_punctuation=False)\n",
    "\n",
    "\n",
    "print(\"APPLYING TEXT PREPROCESSING\")\n",
    "\n",
    "\n",
    "df_clean['text_processed'] = df_clean['clause_text'].apply(preprocess_text_for_tfidf)\n",
    "\n",
    "print(f\"\\n[OK] Preprocessing applied to {len(df_clean)} clauses\")\n",
    "print(f\"\\nSample original vs processed:\")\n",
    "for i in range(min(3, len(df_clean))):\n",
    "    orig = df_clean.iloc[i]['clause_text'][:100]\n",
    "    proc = df_clean.iloc[i]['text_processed'][:100]\n",
    "    print(f\"\\n  Original:  {orig}...\")\n",
    "    print(f\"  Processed: {proc}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Label Encoding and Category Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL ENCODING AND CATEGORY PREPARATION\n",
      "\n",
      "Strategy: Select 8-10 common categories, group remaining as 'Other'\n",
      "\n",
      "Selected top 9 common categories:\n",
      "   1. Parties                                  :  1577 clauses (14.28%)\n",
      "   2. License Grant                            :   658 clauses ( 5.96%)\n",
      "   3. Audit Rights                             :   633 clauses ( 5.73%)\n",
      "   4. Anti-Assignment                          :   614 clauses ( 5.56%)\n",
      "   5. Insurance                                :   550 clauses ( 4.98%)\n",
      "   6. Cap On Liability                         :   545 clauses ( 4.93%)\n",
      "   7. Governing Law                            :   453 clauses ( 4.10%)\n",
      "   8. Agreement Date                           :   434 clauses ( 3.93%)\n",
      "   9. Expiration Date                          :   415 clauses ( 3.76%)\n",
      "\n",
      "  Remaining categories: 32 categories\n",
      "  Total clauses in remaining: 5168 (46.78%)\n",
      "  -> Will be grouped as 'Other' category\n",
      "\n",
      "[OK] Category grouping completed:\n",
      "  Total categories after grouping: 10\n",
      "  Category distribution:\n",
      "    Other                                    :  5168 clauses (46.78%)\n",
      "    Parties                                  :  1577 clauses (14.28%)\n",
      "    License Grant                            :   658 clauses ( 5.96%)\n",
      "    Audit Rights                             :   633 clauses ( 5.73%)\n",
      "    Anti-Assignment                          :   614 clauses ( 5.56%)\n",
      "    Insurance                                :   550 clauses ( 4.98%)\n",
      "    Cap On Liability                         :   545 clauses ( 4.93%)\n",
      "    Governing Law                            :   453 clauses ( 4.10%)\n",
      "    Agreement Date                           :   434 clauses ( 3.93%)\n",
      "    Expiration Date                          :   415 clauses ( 3.76%)\n",
      "\n",
      "[OK] Label encoding completed:\n",
      "  10 categories encoded to integers 0-9\n",
      "\n",
      "  Label mapping:\n",
      "     0 -> Agreement Date                           (  434 samples)\n",
      "     1 -> Anti-Assignment                          (  614 samples)\n",
      "     2 -> Audit Rights                             (  633 samples)\n",
      "     3 -> Cap On Liability                         (  545 samples)\n",
      "     4 -> Expiration Date                          (  415 samples)\n",
      "     5 -> Governing Law                            (  453 samples)\n",
      "     6 -> Insurance                                (  550 samples)\n",
      "     7 -> License Grant                            (  658 samples)\n",
      "     8 -> Other                                    ( 5168 samples)\n",
      "     9 -> Parties                                  ( 1577 samples)\n",
      "\n",
      "[OK] Number of classes for models: 10 (including 'Other')\n"
     ]
    }
   ],
   "source": [
    "print(\"LABEL ENCODING AND CATEGORY PREPARATION\")\n",
    "print(\"\\nStrategy: Select 8-10 common categories, group remaining as 'Other'\")\n",
    "\n",
    "top_n_categories = 9\n",
    "category_counts_sorted = df_clean['category'].value_counts()\n",
    "top_categories = category_counts_sorted.head(top_n_categories)\n",
    "\n",
    "print(f\"\\nSelected top {top_n_categories} common categories:\")\n",
    "for i, (cat, count) in enumerate(top_categories.items(), 1):\n",
    "    pct = (count / len(df_clean)) * 100\n",
    "    print(f\"  {i:2d}. {cat:40s} : {count:5d} clauses ({pct:5.2f}%)\")\n",
    "\n",
    "remaining_categories = category_counts_sorted.iloc[top_n_categories:]\n",
    "remaining_count = remaining_categories.sum()\n",
    "remaining_pct = (remaining_count / len(df_clean)) * 100\n",
    "\n",
    "print(f\"\\n  Remaining categories: {len(remaining_categories)} categories\")\n",
    "print(f\"  Total clauses in remaining: {remaining_count} ({remaining_pct:.2f}%)\")\n",
    "print(f\"  -> Will be grouped as 'Other' category\")\n",
    "\n",
    "df_final = df_clean.copy()\n",
    "\n",
    "df_final['category_grouped'] = df_final['category'].apply(\n",
    "    lambda x: x if x in top_categories.index else 'Other'\n",
    ")\n",
    "\n",
    "print(f\"\\n[OK] Category grouping completed:\")\n",
    "print(f\"  Total categories after grouping: {df_final['category_grouped'].nunique()}\")\n",
    "print(f\"  Category distribution:\")\n",
    "category_grouped_counts = df_final['category_grouped'].value_counts()\n",
    "for cat, count in category_grouped_counts.items():\n",
    "    pct = (count / len(df_final)) * 100\n",
    "    print(f\"    {cat:40s} : {count:5d} clauses ({pct:5.2f}%)\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_final['label_encoded'] = label_encoder.fit_transform(df_final['category_grouped'])\n",
    "\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "print(f\"\\n[OK] Label encoding completed:\")\n",
    "print(f\"  {len(label_mapping)} categories encoded to integers 0-{len(label_mapping)-1}\")\n",
    "print(f\"\\n  Label mapping:\")\n",
    "for label, cat in sorted(reverse_label_mapping.items()):\n",
    "    count = (df_final['category_grouped'] == cat).sum()\n",
    "    print(f\"    {label:2d} -> {cat:40s} ({count:5d} samples)\")\n",
    "\n",
    "num_classes = len(label_mapping)\n",
    "print(f\"\\n[OK] Number of classes for models: {num_classes} (including 'Other')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN/TEST DATA PREPARATION\n",
      "\n",
      "Strategy:\n",
      "  - Training data: CUADv1.json (all data, validation split will be used during training)\n",
      "  - Test data: test.json (separate holdout set for final unbiased evaluation)\n",
      "\n",
      "Training data from CUADv1.json:\n",
      "  Total clauses: 11047\n",
      "  X_train: 11047 clauses\n",
      "  y_train: 11047 labels\n",
      "\n",
      "Extracting test data from test.json...\n",
      "Processing 102 contracts...\n",
      "\n",
      "[OK] Extracted 2643 clauses from 102 contracts\n",
      "   Unique categories: 40\n",
      "   Unique contracts: 102\n",
      "  Raw test clauses extracted: 2643\n",
      "  X_test: 2172 clauses (after filtering and grouping)\n",
      "  y_test: 2172 labels\n",
      "\n",
      "Test set category distribution:\n",
      "   0 (Agreement Date                     ):   87 samples ( 4.01%)\n",
      "   1 (Anti-Assignment                    ):  128 samples ( 5.89%)\n",
      "   2 (Audit Rights                       ):  102 samples ( 4.70%)\n",
      "   3 (Cap On Liability                   ):  106 samples ( 4.88%)\n",
      "   4 (Expiration Date                    ):   73 samples ( 3.36%)\n",
      "   5 (Governing Law                      ):   90 samples ( 4.14%)\n",
      "   6 (Insurance                          ):  116 samples ( 5.34%)\n",
      "   7 (License Grant                      ):  107 samples ( 4.93%)\n",
      "   8 (Other                              ):  998 samples (45.95%)\n",
      "   9 (Parties                            ):  365 samples (16.80%)\n",
      "\n",
      "[OK] Using test.json as final test set\n",
      "\n",
      "Final data split:\n",
      "  Training set: 11047 clauses\n",
      "  Test set: 2172 clauses\n",
      "  Training percentage: 83.6%\n",
      "  Test percentage: 16.4%\n",
      "\n",
      "Class weights calculated (for imbalanced handling):\n",
      "   0 (Agreement Date                     ): 2.545\n",
      "   1 (Anti-Assignment                    ): 1.799\n",
      "   2 (Audit Rights                       ): 1.745\n",
      "   3 (Cap On Liability                   ): 2.027\n",
      "   4 (Expiration Date                    ): 2.662\n",
      "  ... (showing first 5, 10 total)\n",
      "\n",
      "Labels prepared for multi-class classification:\n",
      "  y_train: (11047,) (integer labels, 0 to 9)\n",
      "  y_test: (2172,) (integer labels)\n",
      "  Loss function: Sparse Categorical Crossentropy\n",
      "  Output activation: Softmax\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN/TEST DATA PREPARATION\")\n",
    "print(\"\\nStrategy:\")\n",
    "print(\"  - Training data: CUADv1.json (all data, validation split will be used during training)\")\n",
    "print(\"  - Test data: test.json (separate holdout set for final unbiased evaluation)\")\n",
    "\n",
    "print(f\"\\nTraining data from CUADv1.json:\")\n",
    "print(f\"  Total clauses: {len(df_final)}\")\n",
    "\n",
    "X_train = df_final['text_processed'].values\n",
    "y_train = df_final['label_encoded'].values\n",
    "\n",
    "print(f\"  X_train: {len(X_train)} clauses\")\n",
    "print(f\"  y_train: {len(y_train)} labels\")\n",
    "\n",
    "print(f\"\\nExtracting test data from test.json...\")\n",
    "test_file = data_path / \"test.json\"\n",
    "\n",
    "if test_file.exists():\n",
    "    test_clauses_df = extract_clauses_from_cuadv1(test_file)\n",
    "\n",
    "    if not test_clauses_df.empty:\n",
    "        print(f\"  Raw test clauses extracted: {len(test_clauses_df)}\")\n",
    "\n",
    "        test_clauses_df = test_clauses_df[test_clauses_df['clause_text'].str.strip() != '']\n",
    "        test_clauses_df = test_clauses_df[test_clauses_df['word_count'] >= 2]\n",
    "        test_clauses_df = test_clauses_df.drop_duplicates(subset=['clause_text'], keep='first')\n",
    "\n",
    "        test_clauses_df['text_processed'] = test_clauses_df['clause_text'].apply(preprocess_text_for_tfidf)\n",
    "\n",
    "        test_clauses_df['category_grouped'] = test_clauses_df['category'].apply(\n",
    "            lambda x: x if x in top_categories.index else 'Other'\n",
    "        )\n",
    "\n",
    "        test_clauses_df = test_clauses_df[test_clauses_df['category_grouped'].isin(label_encoder.classes_)]\n",
    "\n",
    "        test_clauses_df['label_encoded'] = label_encoder.transform(test_clauses_df['category_grouped'])\n",
    "\n",
    "        X_test = test_clauses_df['text_processed'].values\n",
    "        y_test = test_clauses_df['label_encoded'].values\n",
    "\n",
    "        print(f\"  X_test: {len(X_test)} clauses (after filtering and grouping)\")\n",
    "        print(f\"  y_test: {len(y_test)} labels\")\n",
    "\n",
    "        print(f\"\\nTest set category distribution:\")\n",
    "        test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "        for label, count in test_dist.items():\n",
    "            cat_name = reverse_label_mapping[label]\n",
    "            pct = (count / len(y_test)) * 100 if len(y_test) > 0 else 0\n",
    "            print(f\"  {label:2d} ({cat_name[:35]:35s}): {count:4d} samples ({pct:5.2f}%)\")\n",
    "\n",
    "        print(f\"\\n[OK] Using test.json as final test set\")\n",
    "\n",
    "    else:\n",
    "        print(\"  WARNING: No clauses extracted from test.json\")\n",
    "        print(\"  Falling back to train/test split from CUADv1.json\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_train, y_train,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=y_train,\n",
    "            shuffle=True\n",
    "        )\n",
    "        print(f\"  Split: {len(X_train)} train, {len(X_test)} test\")\n",
    "else:\n",
    "    print(f\"  WARNING: test.json not found at {test_file}\")\n",
    "    print(\"  Falling back to train/test split from CUADv1.json\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_train,\n",
    "        shuffle=True\n",
    "    )\n",
    "    print(f\"  Split: {len(X_train)} train, {len(X_test)} test\")\n",
    "\n",
    "print(f\"\\nFinal data split:\")\n",
    "print(f\"  Training set: {len(X_train)} clauses\")\n",
    "print(f\"  Test set: {len(X_test)} clauses\")\n",
    "print(f\"  Training percentage: {len(X_train)/(len(X_train)+len(X_test))*100:.1f}%\" if (len(X_train)+len(X_test)) > 0 else \"\")\n",
    "print(f\"  Test percentage: {len(X_test)/(len(X_train)+len(X_test))*100:.1f}%\" if (len(X_train)+len(X_test)) > 0 else \"\")\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights_dict = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_dict))\n",
    "\n",
    "print(f\"\\nClass weights calculated (for imbalanced handling):\")\n",
    "for label, weight in list(class_weights.items())[:5]:\n",
    "    cat_name = reverse_label_mapping[label]\n",
    "    print(f\"  {label:2d} ({cat_name[:35]:35s}): {weight:.3f}\")\n",
    "print(f\"  ... (showing first 5, {len(class_weights)} total)\")\n",
    "\n",
    "print(f\"\\nLabels prepared for multi-class classification:\")\n",
    "print(f\"  y_train: {y_train.shape} (integer labels, 0 to {num_classes-1})\")\n",
    "print(f\"  y_test: {y_test.shape} (integer labels)\")\n",
    "print(f\"  Loss function: Sparse Categorical Crossentropy\")\n",
    "print(f\"  Output activation: Softmax\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Prepare Data for Feedforward Model (TF-IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING TF-IDF FEATURES FOR FEEDFORWARD NEURAL NETWORK (MLP)\n",
      "\n",
      "Model Architecture Plan:\n",
      "  - Input: TF-IDF vectors (dense arrays)\n",
      "  - Hidden layers: 1-2 layers with ReLU activation\n",
      "  - Output: Softmax activation for multi-class classification\n",
      "  - Loss: Sparse Categorical Crossentropy\n",
      "  - Optimizer: Adam\n",
      "\n",
      "Fitting TF-IDF vectorizer on training data...\n",
      "\n",
      "Converting sparse matrices to dense arrays...\n",
      "\n",
      "[OK] TF-IDF transformation completed:\n",
      "  Training features shape: (11047, 5000)\n",
      "  Test features shape: (2172, 5000)\n",
      "  Vocabulary size: 5000\n",
      "  Actual features: 5000\n",
      "  Sparsity: 99.56% (converted to dense)\n",
      "\n",
      "[OK] Data ready for Feedforward Neural Network (MLP) with TF-IDF!\n",
      "   Input shape: 5000 features\n",
      "   Output shape: 10 classes (softmax)\n"
     ]
    }
   ],
   "source": [
    "print(\"PREPARING TF-IDF FEATURES FOR FEEDFORWARD NEURAL NETWORK (MLP)\")\n",
    "print(\"\\nModel Architecture Plan:\")\n",
    "print(\"  - Input: TF-IDF vectors (dense arrays)\")\n",
    "print(\"  - Hidden layers: 1-2 layers with ReLU activation\")\n",
    "print(\"  - Output: Softmax activation for multi-class classification\")\n",
    "print(\"  - Loss: Sparse Categorical Crossentropy\")\n",
    "print(\"  - Optimizer: Adam\")\n",
    "\n",
    "max_features = 5000\n",
    "min_df = 2\n",
    "max_df = 0.95\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    min_df=min_df,\n",
    "    max_df=max_df,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "print(f\"\\nFitting TF-IDF vectorizer on training data...\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\nConverting sparse matrices to dense arrays...\")\n",
    "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
    "X_test_tfidf_dense = X_test_tfidf.toarray()\n",
    "\n",
    "print(f\"\\n[OK] TF-IDF transformation completed:\")\n",
    "print(f\"  Training features shape: {X_train_tfidf_dense.shape}\")\n",
    "print(f\"  Test features shape: {X_test_tfidf_dense.shape}\")\n",
    "print(f\"  Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Actual features: {X_train_tfidf_dense.shape[1]}\")\n",
    "\n",
    "sparsity_train = (1.0 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100\n",
    "print(f\"  Sparsity: {sparsity_train:.2f}% (converted to dense)\")\n",
    "\n",
    "print(f\"\\n[OK] Data ready for Feedforward Neural Network (MLP) with TF-IDF!\")\n",
    "print(f\"   Input shape: {X_train_tfidf_dense.shape[1]} features\")\n",
    "print(f\"   Output shape: {num_classes} classes (softmax)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use tfidf with traditional models\n",
    "to get sequential info use static embedding(try)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Prepare Data for LSTM Model (Tokenization & Padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING SEQUENCE DATA FOR LSTM-BASED TEXT CLASSIFIER\n",
      "\n",
      "Model Architecture Plan:\n",
      "  - Input: Tokenized sequences (word IDs)\n",
      "  - Embedding: Random embeddings (or pretrained)\n",
      "  - LSTM/BiLSTM: Capture word order and context\n",
      "  - Output: Softmax activation for multi-class classification\n",
      "  - Loss: Sparse Categorical Crossentropy\n",
      "  - Optimizer: Adam\n",
      "\n",
      "Why LSTM: Captures word order important for legal phrases like\n",
      "  'subject to', 'shall remain', 'governing law shall be...'\n",
      "\n",
      "Sequence length analysis:\n",
      "  95th percentile: 118 words\n",
      "  99th percentile: 207 words\n",
      "  Selected max_sequence_length: 138\n",
      "\n",
      "Fitting tokenizer on training data...\n",
      "  Actual vocabulary size: 9,833 words\n",
      "  Using top 9,833 words\n",
      "\n",
      "Converting texts to sequences...\n",
      "Padding sequences to length 138...\n",
      "\n",
      "[OK] Sequence preparation completed:\n",
      "  Training sequences shape: (11047, 138)\n",
      "  Test sequences shape: (2172, 138)\n",
      "  Max sequence length: 138\n",
      "  Vocabulary size: 9,833\n",
      "\n",
      "Truncation statistics:\n",
      "  Train sequences truncated: 397 (3.59%)\n",
      "  Test sequences truncated: 77 (3.55%)\n",
      "\n",
      "[OK] Data ready for LSTM-Based Text Classifier!\n",
      "   Input shape: (138,) - sequences of word IDs\n",
      "   Embedding input dim: 9,833\n",
      "   Output shape: 10 classes (softmax)\n"
     ]
    }
   ],
   "source": [
    "print(\"PREPARING SEQUENCE DATA FOR LSTM-BASED TEXT CLASSIFIER\")\n",
    "print(\"\\nModel Architecture Plan:\")\n",
    "print(\"  - Input: Tokenized sequences (word IDs)\")\n",
    "print(\"  - Embedding: Random embeddings (or pretrained)\")\n",
    "print(\"  - LSTM/BiLSTM: Capture word order and context\")\n",
    "print(\"  - Output: Softmax activation for multi-class classification\")\n",
    "print(\"  - Loss: Sparse Categorical Crossentropy\")\n",
    "print(\"  - Optimizer: Adam\")\n",
    "print(\"\\nWhy LSTM: Captures word order important for legal phrases like\")\n",
    "print(\"  'subject to', 'shall remain', 'governing law shall be...'\")\n",
    "\n",
    "word_counts = [len(text.split()) for text in X_train]\n",
    "p95_length = int(np.percentile(word_counts, 95))\n",
    "p99_length = int(np.percentile(word_counts, 99))\n",
    "max_sequence_length = min(p95_length + 20, 300)\n",
    "\n",
    "print(f\"\\nSequence length analysis:\")\n",
    "print(f\"  95th percentile: {p95_length} words\")\n",
    "print(f\"  99th percentile: {p99_length} words\")\n",
    "print(f\"  Selected max_sequence_length: {max_sequence_length}\")\n",
    "\n",
    "vocab_size = 10000\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=vocab_size,\n",
    "    oov_token='<OOV>',\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' '\n",
    ")\n",
    "\n",
    "print(f\"\\nFitting tokenizer on training data...\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "actual_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"  Actual vocabulary size: {actual_vocab_size:,} words\")\n",
    "print(f\"  Using top {min(vocab_size, actual_vocab_size):,} words\")\n",
    "\n",
    "print(f\"\\nConverting texts to sequences...\")\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(f\"Padding sequences to length {max_sequence_length}...\")\n",
    "X_train_padded = pad_sequences(\n",
    "    X_train_seq,\n",
    "    maxlen=max_sequence_length,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "X_test_padded = pad_sequences(\n",
    "    X_test_seq,\n",
    "    maxlen=max_sequence_length,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "print(f\"\\n[OK] Sequence preparation completed:\")\n",
    "print(f\"  Training sequences shape: {X_train_padded.shape}\")\n",
    "print(f\"  Test sequences shape: {X_test_padded.shape}\")\n",
    "print(f\"  Max sequence length: {max_sequence_length}\")\n",
    "print(f\"  Vocabulary size: {actual_vocab_size:,}\")\n",
    "\n",
    "train_truncated = sum([len(seq) > max_sequence_length for seq in X_train_seq])\n",
    "test_truncated = sum([len(seq) > max_sequence_length for seq in X_test_seq])\n",
    "\n",
    "print(f\"\\nTruncation statistics:\")\n",
    "print(f\"  Train sequences truncated: {train_truncated} ({train_truncated/len(X_train_seq)*100:.2f}%)\")\n",
    "print(f\"  Test sequences truncated: {test_truncated} ({test_truncated/len(X_test_seq)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n[OK] Data ready for LSTM-Based Text Classifier!\")\n",
    "print(f\"   Input shape: ({max_sequence_length},) - sequences of word IDs\")\n",
    "print(f\"   Embedding input dim: {actual_vocab_size:,}\")\n",
    "print(f\"   Output shape: {num_classes} classes (softmax)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Preprocessing Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING SUMMARY - READY FOR MODEL CREATION\n",
      "\n",
      "[OK] Data Preprocessing Complete for Multi-Class Classification!\n",
      "\n",
      " Dataset Overview:\n",
      "   - Original clauses: 13,823\n",
      "   - After cleaning: 11,047\n",
      "   - Selected categories: 10 (top categories)\n",
      "   - Train samples: 11,047\n",
      "   - Test samples: 2,172\n",
      "\n",
      "\n",
      "\n",
      " Model 1: Feedforward Neural Network (MLP) with TF-IDF\n",
      "\n",
      "   - X_train_tfidf_dense: (11047, 5000)\n",
      "   - X_test_tfidf_dense: (2172, 5000)\n",
      "   - Features: 5,000 TF-IDF features\n",
      "   - Architecture: Input(5000) -> Hidden(ReLU) -> Output(10, Softmax)\n",
      "\n",
      "\n",
      "\n",
      " Model 2: LSTM-Based Text Classifier\n",
      "\n",
      "   - X_train_padded: (11047, 138)\n",
      "   - X_test_padded: (2172, 138)\n",
      "   - Vocabulary size: 9,833\n",
      "   - Max sequence length: 138\n",
      "   - Architecture: Input(138) -> Embedding(9,833) -> LSTM -> Output(10, Softmax)\n",
      "\n",
      "\n",
      "\n",
      " Labels (Both Models):\n",
      "   - y_train: (11047,) (integer labels: 0 to 9)\n",
      "   - y_test: (2172,) (integer labels)\n",
      "   - Loss: Sparse Categorical Crossentropy\n",
      "   - Output: Softmax (10 classes)\n",
      "   - Class weights: Computed for imbalanced handling\n",
      "\n",
      " Ready to create models!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"PREPROCESSING SUMMARY - READY FOR MODEL CREATION\")\n",
    "\n",
    "print(f\"\"\"\n",
    "[OK] Data Preprocessing Complete for Multi-Class Classification!\n",
    "\n",
    " Dataset Overview:\n",
    "   - Original clauses: {len(clauses_df):,}\n",
    "   - After cleaning: {len(df_final):,}\n",
    "   - Selected categories: {num_classes} (top categories)\n",
    "   - Train samples: {len(X_train):,}\n",
    "   - Test samples: {len(X_test):,}\n",
    "\n",
    "\n",
    "\n",
    " Model 1: Feedforward Neural Network (MLP) with TF-IDF\n",
    "\n",
    "   - X_train_tfidf_dense: {X_train_tfidf_dense.shape}\n",
    "   - X_test_tfidf_dense: {X_test_tfidf_dense.shape}\n",
    "   - Features: {X_train_tfidf_dense.shape[1]:,} TF-IDF features\n",
    "   - Architecture: Input({X_train_tfidf_dense.shape[1]}) -> Hidden(ReLU) -> Output({num_classes}, Softmax)\n",
    "\n",
    "\n",
    "\n",
    " Model 2: LSTM-Based Text Classifier\n",
    "\n",
    "   - X_train_padded: {X_train_padded.shape}\n",
    "   - X_test_padded: {X_test_padded.shape}\n",
    "   - Vocabulary size: {actual_vocab_size:,}\n",
    "   - Max sequence length: {max_sequence_length}\n",
    "   - Architecture: Input({max_sequence_length}) -> Embedding({actual_vocab_size:,}) -> LSTM -> Output({num_classes}, Softmax)\n",
    "\n",
    "\n",
    "\n",
    " Labels (Both Models):\n",
    "   - y_train: {y_train.shape} (integer labels: 0 to {num_classes-1})\n",
    "   - y_test: {y_test.shape} (integer labels)\n",
    "   - Loss: Sparse Categorical Crossentropy\n",
    "   - Output: Softmax ({num_classes} classes)\n",
    "   - Class weights: Computed for imbalanced handling\n",
    "\n",
    " Ready to create models!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save prepared data\n",
    "\n",
    "Write clean dataset and model-ready arrays to `data/processed/` so the main classification notebook can load them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /Users/khajamoinuddinmohammed/Documents/MSDS/FALL 2025/BUAN 5312 ADVANCED ML/final project/data/processed\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1) Clean clause-level dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m clauses_clean_path \u001b[38;5;241m=\u001b[39m processed_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclauses_clean.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdf_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclauses_clean_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclauses_clean_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2) NumPy arrays\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/frame.py:3124\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3045\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parquet.py:478\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    477\u001b[0m     partition_cols \u001b[38;5;241m=\u001b[39m [partition_cols]\n\u001b[0;32m--> 478\u001b[0m impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m    482\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    483\u001b[0m     df,\n\u001b[1;32m    484\u001b[0m     path_or_buf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    491\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/parquet.py:68\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     66\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# Output directory (under project root)\n",
    "processed_dir = project_root / \"data\" / \"processed\"\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving to {processed_dir.absolute()}\")\n",
    "\n",
    "# 1) Clean clause-level dataset\n",
    "clauses_clean_path = processed_dir / \"clauses_clean.csv\"\n",
    "df_final.to_csv(clauses_clean_path, index=False)\n",
    "print(f\"  - {clauses_clean_path.name}\")\n",
    "\n",
    "# 2) NumPy arrays\n",
    "np.save(processed_dir / \"X_train_tfidf_dense.npy\", X_train_tfidf_dense)\n",
    "np.save(processed_dir / \"X_test_tfidf_dense.npy\", X_test_tfidf_dense)\n",
    "np.save(processed_dir / \"X_train_padded.npy\", X_train_padded)\n",
    "np.save(processed_dir / \"X_test_padded.npy\", X_test_padded)\n",
    "np.save(processed_dir / \"y_train.npy\", y_train)\n",
    "np.save(processed_dir / \"y_test.npy\", y_test)\n",
    "print(\"  - X_train_tfidf_dense.npy, X_test_tfidf_dense.npy\")\n",
    "print(\"  - X_train_padded.npy, X_test_padded.npy\")\n",
    "print(\"  - y_train.npy, y_test.npy\")\n",
    "\n",
    "# 3) Sklearn objects (joblib)\n",
    "import joblib\n",
    "joblib.dump(label_encoder, processed_dir / \"label_encoder.joblib\")\n",
    "joblib.dump(tfidf_vectorizer, processed_dir / \"tfidf_vectorizer.joblib\")\n",
    "print(\"  - label_encoder.joblib, tfidf_vectorizer.joblib\")\n",
    "\n",
    "# 4) Keras Tokenizer (pickle)\n",
    "with open(processed_dir / \"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"  - tokenizer.pkl\")\n",
    "\n",
    "# 5) Config (num_classes, max_sequence_length, vocab_size, reverse_label_mapping, class_weights)\n",
    "config = {\n",
    "    \"num_classes\": num_classes,\n",
    "    \"max_sequence_length\": int(max_sequence_length),\n",
    "    \"vocab_size\": int(actual_vocab_size),\n",
    "    \"reverse_label_mapping\": {str(k): v for k, v in reverse_label_mapping.items()},\n",
    "    \"class_weights\": {str(k): float(v) for k, v in class_weights.items()},\n",
    "}\n",
    "with open(processed_dir / \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"  - config.json\")\n",
    "\n",
    "print(\"\\n[OK] Prepared data saved. Run the main classification notebook and load from data/processed/.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
